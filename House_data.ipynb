{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, max, min, count, year, month\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import logging\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2.credentials import Credentials\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/vboxuser/.local/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/vboxuser/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-api-python-client in /home/vboxuser/.local/lib/python3.10/site-packages (2.148.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client) (0.20.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-api-python-client) (2.20.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-api-python-client) (2.35.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.65.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/vboxuser/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/vboxuser/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/15 12:30:03 WARN Utils: Your hostname, tce resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/15 12:30:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/15 12:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the CSV file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully!\n",
      "DataFrame Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- sqft_living: string (nullable = true)\n",
      " |-- sqft_lot: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- waterfront: string (nullable = true)\n",
      " |-- view: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- yr_built: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "+----------+---------------+---------+--------+---------+-----------+--------+------+----------+-------+---------+-----+--------+-------+-------+--------+\n",
      "|        id|           date|    price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|   view|condition|grade|yr_built|zipcode|    lat|    long|\n",
      "+----------+---------------+---------+--------+---------+-----------+--------+------+----------+-------+---------+-----+--------+-------+-------+--------+\n",
      "|        id|           date|     NULL|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|   view|condition|grade|yr_built|zipcode|    lat|    long|\n",
      "| 629000615|20141022T000000|1495000.0|       4|     3.25|       3070|   10375|     2|        No|No View|  Average|   10|    1962|  98004|47.5862|-122.198|\n",
      "|4141800215|20141126T000000|1495000.0|       4|     3.75|       3770|    4000|   2.5|        No|No View|Very Good|    9|    1916|  98122|47.6157|-122.287|\n",
      "|6448000020|20150129T000000|1490000.0|       4|      2.5|       2420|   18480|     1|        No|No View|     Good|    9|    1967|  98004|47.6214|-122.227|\n",
      "|9522300010|20150331T000000|1490000.0|       3|      3.5|       4560|   14608|     2|        No|Average|  Average|   12|    1990|  98034|47.6995|-122.228|\n",
      "+----------+---------------+---------+--------+---------+-----------+--------+------+----------+-------+---------+-----+--------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating price statistics by zipcode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price statistics saved to /home/vboxuser/airflow/dags/analytics/price_stats_by_zipcode\n",
      "Analyzing house features...\n",
      "House features analysis saved to /home/vboxuser/airflow/dags/analytics/house_features_analysis\n",
      "Analyzing condition and grade distribution...\n",
      "Condition and grade distribution saved to /home/vboxuser/airflow/dags/analytics/condition_distribution\n",
      "Analyzing waterfront properties...\n",
      "Waterfront analysis saved to /home/vboxuser/airflow/dags/analytics/waterfront_analysis\n",
      "Consolidating CSV files from /home/vboxuser/airflow/dags/analytics/price_stats_by_zipcode into /home/vboxuser/airflow/dags/analytics/price_stats_by_zipcode_final.csv...\n",
      "Consolidated files into /home/vboxuser/airflow/dags/analytics/price_stats_by_zipcode_final.csv\n",
      "Consolidating CSV files from /home/vboxuser/airflow/dags/analytics/house_features_analysis into /home/vboxuser/airflow/dags/analytics/house_features_analysis_final.csv...\n",
      "Consolidated files into /home/vboxuser/airflow/dags/analytics/house_features_analysis_final.csv\n",
      "Consolidating CSV files from /home/vboxuser/airflow/dags/analytics/condition_distribution into /home/vboxuser/airflow/dags/analytics/condition_distribution_final.csv...\n",
      "Consolidated files into /home/vboxuser/airflow/dags/analytics/condition_distribution_final.csv\n",
      "Consolidating CSV files from /home/vboxuser/airflow/dags/analytics/waterfront_analysis into /home/vboxuser/airflow/dags/analytics/waterfront_analysis_final.csv...\n",
      "Consolidated files into /home/vboxuser/airflow/dags/analytics/waterfront_analysis_final.csv\n",
      "Integrating with Google Drive...\n",
      "[\u001b[34m2024-10-15T12:30:31.761+0530\u001b[0m] {\u001b[34m__init__.py:\u001b[0m49} INFO\u001b[0m - file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "Google Drive authentication successful!\n",
      "Deleting old files from Google Drive folder...\n",
      "Deleted file: poverty_rates_by_county_final.csv with ID: 1OsLbz1NQJwXZ-Pqw3Gf3aavNCiU6n3ww\n",
      "Deleted file: employment_statistics_final.csv with ID: 1hKH7qryhXF7UaQnczb5dlTGQaC_dNgen\n",
      "Deleted file: population_distribution_by_county_final.csv with ID: 1SDiYJO-l_bgJeTPH_hirSy54mlnpGCL3\n",
      "Deleted file: income_stats_by_state_final.csv with ID: 1q8RI7YxzK0xTcsOrbAoVl8arnIxWPRBb\n",
      "[\u001b[34m2024-10-15T12:30:43.831+0530\u001b[0m] {\u001b[34mhttp.py:\u001b[0m140} \u001b[33mWARNING\u001b[0m - \u001b[33mEncountered 403 Forbidden with reason \"insufficientFilePermissions\"\u001b[0m\n",
      "Error deleting files: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1t1u03ZTia-wSW2Y92aorcd9qEFKTHR5X? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "Uploading consolidated CSV files to Google Drive...\n",
      "Uploaded price_stats_by_zipcode_final.csv to Google Drive.\n",
      "Uploaded house_features_analysis_final.csv to Google Drive.\n",
      "Uploaded condition_distribution_final.csv to Google Drive.\n",
      "Uploaded waterfront_analysis_final.csv to Google Drive.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, max, min, count, regexp_replace, col\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "def perform_analytics():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"HouseDataAnalytics\").getOrCreate()\n",
    "\n",
    "    file_path = '/home/vboxuser/airflow/dags/house_data.dat'\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Read CSV with custom schema (remapping column names)\n",
    "    print(\"Reading the CSV file...\")\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=False, inferSchema=True, escape='\"')\n",
    "        df = df.withColumnRenamed('_c0', 'id') \\\n",
    "               .withColumnRenamed('_c1', 'date') \\\n",
    "               .withColumnRenamed('_c2', 'price') \\\n",
    "               .withColumnRenamed('_c3', 'bedrooms') \\\n",
    "               .withColumnRenamed('_c4', 'bathrooms') \\\n",
    "               .withColumnRenamed('_c5', 'sqft_living') \\\n",
    "               .withColumnRenamed('_c6', 'sqft_lot') \\\n",
    "               .withColumnRenamed('_c7', 'floors') \\\n",
    "               .withColumnRenamed('_c8', 'waterfront') \\\n",
    "               .withColumnRenamed('_c9', 'view') \\\n",
    "               .withColumnRenamed('_c10', 'condition') \\\n",
    "               .withColumnRenamed('_c11', 'grade') \\\n",
    "               .withColumnRenamed('_c12', 'yr_built') \\\n",
    "               .withColumnRenamed('_c13', 'zipcode') \\\n",
    "               .withColumnRenamed('_c14', 'lat') \\\n",
    "               .withColumnRenamed('_c15', 'long')\n",
    "\n",
    "        # Clean and format the price column (removing commas, quotes, and spaces)\n",
    "        df = df.withColumn('price', regexp_replace(col('price'), '[\", ]', ''))\n",
    "        df = df.withColumn('price', df['price'].cast('double'))\n",
    "        \n",
    "        print(\"CSV file loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Show schema and first few rows to verify data\n",
    "    print(\"DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    print(\"First few rows of the DataFrame:\")\n",
    "    df.show(5)\n",
    "\n",
    "    if df.count() == 0:\n",
    "        print(\"Error: DataFrame is empty. Check the data source.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    analytics_dir = '/home/vboxuser/airflow/dags/analytics'\n",
    "    os.makedirs(analytics_dir, exist_ok=True)\n",
    "\n",
    "    # Define analytics output paths\n",
    "    price_stats_path = os.path.join(analytics_dir, 'price_stats_by_zipcode')\n",
    "    house_features_path = os.path.join(analytics_dir, 'house_features_analysis')\n",
    "    condition_distribution_path = os.path.join(analytics_dir, 'condition_distribution')\n",
    "    waterfront_analysis_path = os.path.join(analytics_dir, 'waterfront_analysis')\n",
    "\n",
    "    # 1. Price statistics by zipcode\n",
    "    print(\"Calculating price statistics by zipcode...\")\n",
    "    try:\n",
    "        price_stats = df.groupBy('zipcode').agg(\n",
    "            mean('price').alias('avg_price'),\n",
    "            max('price').alias('max_price'),\n",
    "            min('price').alias('min_price'),\n",
    "            count('price').alias('house_count')\n",
    "        )\n",
    "        price_stats.write.mode('overwrite').csv(price_stats_path, header=True)\n",
    "        print(f\"Price statistics saved to {price_stats_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating price statistics: {e}\")\n",
    "\n",
    "    # 2. House features analysis\n",
    "    print(\"Analyzing house features...\")\n",
    "    try:\n",
    "        house_features = df.groupBy('bedrooms').agg(\n",
    "            mean('price').alias('avg_price'),\n",
    "            mean('sqft_living').alias('avg_sqft'),\n",
    "            count('*').alias('count')\n",
    "        )\n",
    "        house_features.write.mode('overwrite').csv(house_features_path, header=True)\n",
    "        print(f\"House features analysis saved to {house_features_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing house features: {e}\")\n",
    "\n",
    "    # 3. Condition and grade distribution\n",
    "    print(\"Analyzing condition and grade distribution...\")\n",
    "    try:\n",
    "        condition_distribution = df.groupBy('condition', 'grade').count()\n",
    "        condition_distribution.write.mode('overwrite').csv(condition_distribution_path, header=True)\n",
    "        print(f\"Condition and grade distribution saved to {condition_distribution_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing condition distribution: {e}\")\n",
    "\n",
    "    # 4. Waterfront property analysis\n",
    "    print(\"Analyzing waterfront properties...\")\n",
    "    try:\n",
    "        waterfront_analysis = df.groupBy('waterfront').agg(\n",
    "            mean('price').alias('avg_price'),\n",
    "            count('*').alias('count'),\n",
    "            mean('view').alias('avg_view_score')\n",
    "        )\n",
    "        waterfront_analysis.write.mode('overwrite').csv(waterfront_analysis_path, header=True)\n",
    "        print(f\"Waterfront analysis saved to {waterfront_analysis_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing waterfront properties: {e}\")\n",
    "\n",
    "    # Function to consolidate part files into a single CSV\n",
    "    def consolidate_csv(directory, output_file):\n",
    "        print(f\"Consolidating CSV files from {directory} into {output_file}...\")\n",
    "        try:\n",
    "            with open(output_file, 'wb') as outfile:\n",
    "                for filename in os.listdir(directory):\n",
    "                    if filename.endswith('.csv'):\n",
    "                        file_path = os.path.join(directory, filename)\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            shutil.copyfileobj(f, outfile)\n",
    "            print(f\"Consolidated files into {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error consolidating CSV files: {e}\")\n",
    "\n",
    "    # Consolidate files\n",
    "    consolidate_csv(price_stats_path, os.path.join(analytics_dir, 'price_stats_by_zipcode_final.csv'))\n",
    "    consolidate_csv(house_features_path, os.path.join(analytics_dir, 'house_features_analysis_final.csv'))\n",
    "    consolidate_csv(condition_distribution_path, os.path.join(analytics_dir, 'condition_distribution_final.csv'))\n",
    "    consolidate_csv(waterfront_analysis_path, os.path.join(analytics_dir, 'waterfront_analysis_final.csv'))\n",
    "\n",
    "    # Google Drive integration\n",
    "    print(\"Integrating with Google Drive...\")\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    creds = None\n",
    "    try:\n",
    "        creds = Credentials.from_authorized_user_file('/home/vboxuser/airflow/dags/token.json', SCOPES)\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Google Drive authentication successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error authenticating with Google Drive: {e}\")\n",
    "        return\n",
    "\n",
    "    folder_id = '1_g7WmLJAEKDqSJXGF7e0k5j7WO-LVkRT'\n",
    "\n",
    "    def delete_old_files():\n",
    "        print(\"Deleting old files from Google Drive folder...\")\n",
    "        try:\n",
    "            results = service.files().list(q=f\"'{folder_id}' in parents\", fields=\"files(id, name)\").execute()\n",
    "            items = results.get('files', [])\n",
    "            if not items:\n",
    "                print(\"No old files found to delete.\")\n",
    "            else:\n",
    "                for item in items:\n",
    "                    service.files().delete(fileId=item['id']).execute()\n",
    "                    print(f\"Deleted file: {item['name']} with ID: {item['id']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting files: {e}\")\n",
    "\n",
    "    delete_old_files()\n",
    "\n",
    "    # Upload consolidated CSV files to Google Drive\n",
    "    final_csv_files = [\n",
    "        os.path.join(analytics_dir, 'price_stats_by_zipcode_final.csv'),\n",
    "        os.path.join(analytics_dir, 'house_features_analysis_final.csv'),\n",
    "        os.path.join(analytics_dir, 'condition_distribution_final.csv'),\n",
    "        os.path.join(analytics_dir, 'waterfront_analysis_final.csv')\n",
    "    ]\n",
    "\n",
    "    print(\"Uploading consolidated CSV files to Google Drive...\")\n",
    "    for file_path in final_csv_files:\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".csv\"):\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_metadata = {\n",
    "                'name': file_name,\n",
    "                'parents': [folder_id]\n",
    "            }\n",
    "            media = MediaFileUpload(file_path, mimetype='text/csv')\n",
    "            try:\n",
    "                service.files().create(\n",
    "                    body=file_metadata,\n",
    "                    media_body=media,\n",
    "                    fields='id'\n",
    "                ).execute()\n",
    "                print(f\"Uploaded {file_name} to Google Drive.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "# Call the function to execute the analytics\n",
    "perform_analytics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries from Airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import your analytics function (make sure this is defined somewhere)\n",
    "# from your_module import perform_analytics\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 7, 20),  # Adjust as necessary\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'house_data_analysis',\n",
    "    default_args=default_args,\n",
    "    description='A DAG to perform various analytics on house data',\n",
    "    schedule='0 */2 * * *',  # Runs every 2 hours\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "# Define the task using PythonOperator\n",
    "analytics_task = PythonOperator(\n",
    "    task_id='perform_house_data_analysis',\n",
    "    python_callable=perform_analytics,  # Ensure this function is defined elsewhere\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define task dependencies if needed\n",
    "# For example, if you had another task, you could define it here\n",
    "# another_task = PythonOperator(\n",
    "#     task_id='another_task_id',\n",
    "#     python_callable=another_function,\n",
    "#     dag=dag,\n",
    "# )\n",
    "\n",
    "# If you have multiple tasks, you can set dependencies\n",
    "# analytics_task >> another_task  # This line will make analytics_task run before another_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
